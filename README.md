# Deep Learning From Scratch ðŸ§ 

This repo documents my journey of building **modern AI systems from first principles** â€” following tutorials by Andrej Karpathy, Sentdex, and other top educators, while adding my own experiments.

The goal: to truly understand how neural networks, transformers, and generative models work under the hood â€” without treating them as black boxes.

---

## ðŸ“š Contents

### 1. Micrograd
- A tiny autograd engine and neural net library built from scratch
- Learned how backpropagation works at the lowest level
- Repo: [`micrograd/`](./micrograd)

### 2. Makemore
- A character-level language model that generates names/text
- Built multiple improvements: MLPs â†’ CNNs â†’ RNNs â†’ Transformers
- Repo: [`makemore/`](./makemore)

### 3. Transformers
- Implemented a scaled-down transformer and attention mechanism
- Towards building GPT-like models step by step
- Repo: [`transformer/`](./transformer)

### 4. Personal Projects
- Custom experiments & playgrounds inspired by tutorials
- Repo: [`projects/`](./projects)

---

## ðŸš€ Why This Repo?
- To master AI by coding everything from scratch  
- To build a **portfolio of explainable, reproducible projects**  
- To prepare for the future of AI engineering  

---

## ðŸ›  Tech Stack
- Python, NumPy, PyTorch (for scaling beyond scratch)  
- Jupyter notebooks for interactive exploration  

---

## ðŸ“ˆ Next Steps
- Train a GPT-like model on custom datasets  
- Explore reinforcement learning (Atari / gym environments)  
- Experiment with fine-tuning large pre-trained models